import httpx
import openai
import tiktoken
import json
import uvicorn
import signal
import numpy as np
from pymilvus.orm import utility
from starlette.middleware.cors import CORSMiddleware
from pymilvus import connections, Collection
from transformers import BertTokenizer, BertModel
import torch
import json
from tqdm import tqdm
import torch
from transformers import BertTokenizer, BertModel
from tqdm import tqdm
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from fastapi import FastAPI, WebSocket
from pydantic import BaseModel
from fastapi import FastAPI, UploadFile, File
from extract_feature import feature_bart
from data_preprocess import load_data
from modelscope import AutoModelForCausalLM, AutoTokenizer
import torch
from concurrent.futures import ThreadPoolExecutor  
import asyncio  
torch.manual_seed(0)



path = 'qwen/Qwen-14B-Chat'
tokenizer = AutoTokenizer.from_pretrained(path,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float32, device_map='auto', trust_remote_code=True)
history=None
# connections.connect(alias="default",
#                     uri="https://in01-487e267ef1ccb1e.ali-cn-hangzhou.vectordb.zilliz.com.cn:19530",
#                     token="db_admin:Ng1$f4/<x2eppL}n")
#47.100.166.210:19530 ipbd  milvusÁ¶ªÁ∫øÊï∞ÊçÆÂ∫ì
connections.connect("default", host="47.100.166.210", port="19530", user='root', password='')
# connections.connect("default", host="47.100.166.210", port="19530", user='root', password='')
collection_name = 'chat_qw'
collection = Collection(collection_name)




import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModel
import json
tokenizer_bge = AutoTokenizer.from_pretrained('/home/zhengwen/huzhuoyue/minicpm/model/bge-large-zh-v1.5')
model_bge = AutoModel.from_pretrained('/home/zhengwen/huzhuoyue/minicpm/model/bge-large-zh-v1.5')

def get_embedding(file_path):
    texts = []
    max_length=512
    def process_texts(sentences):
        encoded_input = tokenizer_bge(sentences, padding=True, truncation=True, return_tensors='pt')
        with torch.no_grad():
            model_output = model_bge (**encoded_input)
            sentence_embedding = model_output[0][:, 0]
            print(f"############ 62 ËøôÊòØsentence_embedding.shapeÁöÑÂÄº",sentence_embedding.shape)
        return sentence_embedding.tolist()


    with open(file_path, encoding='utf-8') as f:
        for line in f:
            text = line.strip()
            if len(text)<15:
                continue
            #texts.append(text)
            if len(text) > max_length:  
                split_texts = [text[j:j+max_length] for j in range(0, len(text), max_length)]
                for split_text in split_texts:
                    texts.append(split_text)
            else:  
                texts.append(text)  # ÊñáÊú¨ÈïøÂ∫¶Êú™Ë∂ÖËøá512ÔºåÁõ¥Êé•Ê∑ªÂä†Âà∞textsÂàóË°®‰∏≠  
    texts=[text for text in texts if text]
 
    res_feature=process_texts(texts)

    print("################  res_feature ########")
    # print(res_feature)
    print(f"######  84ËøôÊòØres_feature ÁöÑÂçï‰∏™Âè•Â≠êÁöÑÁâπÂæÅÂêëÈáèÁöÑÈïøÂ∫¶ {len(res_feature[0])} #######3")

    print("##### ok 147 ###")
    print("Total embeddings processed:", len(res_feature))
    with open('insert_feature.json', 'w') as f:
        json.dump(res_feature, f)
        print('ÊèêÂèñÂÆåÊàê')
    return res_feature,texts
    



def get_query_embedding(query_text):

    def process_texts(sentences):
        encoded_input = tokenizer_bge(sentences, padding=True, truncation=True, return_tensors='pt')
        with torch.no_grad():
            model_output = model_bge (**encoded_input)
            sentence_embedding = model_output[0][:, 0]
        return sentence_embedding.tolist()
   
    embeddings_list=process_texts([query_text])
    print(f"*****ËøôÊòØget_query_embedding‰∏≠ÁöÑÈóÆÈ¢òÁöÑembedding****")
    print(embeddings_list)
    # res=[embeddings_list]
    # print(res)
    return embeddings_list   

    # def process_texts(texts):
    #     encoded_inputs = tokenizer_bert(texts, padding=True, truncation=True, return_tensors='pt')
    #     model_outputs = model_bert(**encoded_inputs)
    #     sentence_embeddings = mean_pooling(model_outputs, encoded_inputs['attention_mask'])
    #     sentence_embeddings_norm = sentence_embeddings / sentence_embeddings.norm(p=2, dim=-1, keepdim=True)
    #     sentence_embeddings_norm = sentence_embeddings_norm.squeeze(0)
    #     return sentence_embeddings_norm.cpu().detach().numpy().tolist()
        
    # embeddings_list = []
    # embeddings_list.extend(process_texts([query_text]))
    # print(f"*****ËøôÊòØget_query_embedding‰∏≠ÁöÑÈóÆÈ¢òÁöÑembedding****")
    # print(embeddings_list)
    # # res=[embeddings_list]
    # # print(res)
    # return embeddings_list





def read_embedding():
    #embeddingÂêëÈáèÁöÑÂ≠òÂÇ®Ë∑ØÂæÑ
    embedding_path = 'insert_feature.json'
    with open(embedding_path, 'r') as f:
        embedding = json.load(f)

    #ÊñáÊú¨Êñá‰ª∂Â≠òÂÇ®Ë∑ØÂæÑ
    file_path = 'test.txt'
    texts = []
    with open(file_path, encoding='utf-8') as f:
        for line in f:
            text = line.strip()
            texts.append(text)

    return embedding, texts

def insert_embedding(embedding,texts):

    # print("ËøôÊòØinsert_embeddingÈáåÁöÑÁâπÂæÅÂêëÈáè:",embedding)
    print(f"############ 151 ËøôÊòØembeddingÁöÑÁöÑÈïøÂ∫¶",len(embedding[0]))
    print("ËøôÊòØinsert_embeddingÈáåÁöÑÊñáÊú¨:",texts)
    print(len(embedding),print(len(texts)))
    mr = collection.insert([embedding,texts])

    collection.flush()
    print("ok116")
    return mr.succ_count


def query_db(query_str,topk=8):
    collection.load()
    search_params = {"metric_type": "IP", "params": {"nprobe": 64}}
    
    #query_data = openai.Embedding.create(input=query_str, model=model_embedding)["data"][0]["embedding"]
    query_data =get_query_embedding(query_str)
    print("#############[query_data]##########")
    print([query_data][0])
    results = collection.search(
        data=query_data,
        anns_field="embedding",
        param=search_params,
        limit=topk,
        output_fields=["text"]
    )
    res = []
    for hits in results:
        for hit in hits:
            score = round(hit.distance, 3)
            if score >= 150:
                res.append({
                    "score": score,
                    "text": hit.entity.get('text')
                })
    print("Successfully searched similar texts!")
    print(res)
    return res

def retrieve(query_str):
    limit = 3750
    res = query_db(query_str,topk=15)
    print(f"ËøôÊòØretrieveÁöÑres: {res}")
    prompt_start = (
            "ËØ∑ÁªìÂêà‰∏ãÈù¢ÊàëÁªô‰Ω†ÁöÑÂ∑≤Áü•ÊùêÊñô‰ø°ÊÅØÂõûÁ≠îÊàëÁöÑÈóÆÈ¢òÔºåÂ¶ÇÊûúÊòØÊÄªÁªìÊàñËÄÖÊä•ÂëäÁ±ªÁöÑÈóÆÈ¢òÔºåÂõûÁ≠îË∂äËØ¶ÁªÜË∂äÂ•Ω,‰∏çÂ∞ë‰∫é500Â≠ó.\n\n"+"ÊÆµËêΩ:\n"
    )
    prompt_end = (
         f"\n\nÈóÆÈ¢ò: {query_str}\nÂõûÁ≠î:"
    )
    #ÂèñÂá∫Áõ∏‰ººÂú∞ÂêëÈáè
    contexts =[x['text'] for x in res]
    print(f"ËøôÊòØretrieveÁöÑcontexts: {contexts}")
    prompt = 'None'
    if len(res) == 0:
        #Â¶ÇÊûúÊ≤°Êúâ‰ªéÊï∞ÊçÆÂ∫ì‰∏≠ÊâæÂà∞Áõ∏‰ººÁöÑÊñáÊú¨ÂêëÈáèÁªìÊûú
        prompt = prompt

    #Â¶ÇÊûúÊü•ÊâæÂà∞‰∫ÜÁõ∏‰ººÂêëÈáèËøîÂõûÊñáÊú¨
    else:
        for i in range(0, len(contexts)):
            if len("\n\n---\n\n".join(contexts[:i])) >= limit:
                prompt = (
                        prompt_start +
                        "\n\n---\n\n".join(contexts[:i - 1])+"\n\n"+
                        prompt_end
                )
                break
            elif i == len(contexts) - 1:
                prompt = (
                        prompt_start +
                        "\n\n---\n\n".join(contexts)+"\n\n"+
                        prompt_end
                )
        print(f"ËøôÊòØretrieveÁöÑprompt : {prompt}")
    return prompt

#{"role": "system", "content": "‰Ω†ÊòØ‰∏™Ê†πÊçÆÊèèËø∞‰ø°ÊÅØËøõË°åÂõûÁ≠îÁöÑÊú∫Âô®‰∫∫ÔºåÂ∞ΩÈáè‰ΩøÁî®Ëá™Â∑±ÁöÑËØùÊúØ"},
def enhance_chatgpt(prompt):
    history=None
    res= model.chat(tokenizer,prompt, system="‰Ω†ÊòØ‰∏Ä‰∏™Ê†πÊçÆÂ∑≤Áü•‰ø°ÊÅØËøõË°åÂõûÁ≠îÂíåÊÄªÁªìÂπ∂Ê∂¶Ëâ≤Á≠îÊ°àÁöÑÊú∫Âô®‰∫∫ÔºåÁªìÂêàÊùêÊñôÁöÑÊó∂ÂÄôÂ∞ΩÂèØËÉΩ‰øùÁïôÊùêÊñôÔºåÊÄªÁªìÊÄßÁöÑÂõûÁ≠îË∂äËØ¶ÁªÜË∂äÂ•Ω",temperature=0.9, top_p=0.8,history=history)
    return res


#‰Ω†ÊòØ‰∏™Âü∫Á°ÄÈóÆÁ≠îÊú∫Âô®‰∫∫
def ori_chatgpt(query_str):
    history=None
    res, history = model.chat(tokenizer, query_str, system="‰Ω†ÊòØ‰∏™Âü∫Á°ÄÈóÆÁ≠îÊú∫Âô®‰∫∫,ËØ∑‰Ω†Ê†πÊçÆËá™Â∑±ÁöÑ‰ø°ÊÅØËøõË°åÂõûÁ≠îÔºåË∂äËØ¶ÁªÜË∂äÂ•Ω",temperature=0.9, top_p=0.8,history=history)
    return res




app = FastAPI()
# ÂÖÅËÆ∏ÊâÄÊúâÊù•Ê∫êÁöÑË∑®ÂüüËØ∑Ê±Ç
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

#Â¢ûÂº∫ÊèêÈóÆÊé•Âè£
class Item(BaseModel):
    query: str
@app.post('/api/chatgpt/')
def enhance_gpt(item: Item):
    prompt = retrieve(item.query)
    result_enhance= enhance_chatgpt(prompt)
    return {"code": 200, "enhance_qw":result_enhance}


# ÂàõÂª∫WebSocketËøûÊé•ÔºåÂèÇÊï∞‰∏∫ws:WebSocketÂØπË±°
#ÈÄö‰ª•ÂçÉÈóÆÂéüÂßãÂõûÁ≠î
executor_raw = ThreadPoolExecutor()
@app.websocket("/api/raw_socket")  
async def websocket_endpoint(websocket: WebSocket):  
    await websocket.accept()  
    while True:  
        data = await websocket.receive_text()  
        item = Item(**json.loads(data))  

  
        # ‰ΩøÁî®Á∫øÁ®ãÊ±†ÊâßË°åÂô®Êù•ËøêË°åÂêåÊ≠•ÁîüÊàêÂô®  
        loop = asyncio.get_running_loop()  
        async for txt in run_in_executor(executor_raw, model.chat_stream, tokenizer,item.query, history=history, system='‰Ω†ÊòØ‰∏Ä‰∏™Ê†πÊçÆÂ∑≤Áü•‰ø°ÊÅØËøõË°åÂõûÁ≠îÂíåÊÄªÁªìÂπ∂Ê∂¶Ëâ≤Á≠îÊ°àÁöÑÊú∫Âô®‰∫∫ÔºåÁªìÂêàÊùêÊñôÁöÑÊó∂ÂÄôÂ∞ΩÂèØËÉΩ‰øùÁïôÊùêÊñôÔºåÊÄªÁªìÊÄßÁöÑÂõûÁ≠îË∂äËØ¶ÁªÜË∂äÂ•Ω'):  
            await websocket.send_text(txt)  
  
        await websocket.send_json({'ans': 'DONE'})  
        # Â¶ÇÊûúÊâìÁÆóÂú®ÂèëÈÄÅDONEÂêéÁªßÁª≠ÁõëÂê¨Êñ∞ÁöÑËØ∑Ê±ÇÔºåÂ∞±‰∏çË¶ÅÂÖ≥Èó≠websocketËøûÊé•  
        # Â¶ÇÊûúËøôÊòØÂçïÊ¨°ËØ∑Ê±ÇÂìçÂ∫îÊ®°ÂºèÔºåÂàôÂèØ‰ª•ÂÖ≥Èó≠  
        await websocket.close()  



#ÈÄö‰πâÂçÉÈóÆÂ¢ûÂº∫ÂõûÁ≠î¬∑
# @app.websocket('/api/enhance_socket')
# async def enhance_gpt(websocket: WebSocket):
#     await websocket.accept()
#     while True:
#         data = await websocket.receive_text()
#         item = Item(**json.loads(data))
#         prompt = retrieve(item.query)
#         tmp=''
#         # res = model.chat_stream(tokenizer, prompt , history=history, system='‰Ω†ÊòØ‰∏Ä‰∏™Ê†πÊçÆÂ∑≤Áü•‰ø°ÊÅØËøõË°åÂõûÁ≠îÂíåÊÄªÁªìÂπ∂Ê∂¶Ëâ≤Á≠îÊ°àÁöÑÊú∫Âô®‰∫∫ÔºåÁªìÂêàÊùêÊñôÁöÑÊó∂ÂÄôÂ∞ΩÂèØËÉΩ‰øùÁïôÊùêÊñôÔºåÊÄªÁªìÊÄßÁöÑÂõûÁ≠îË∂äËØ¶ÁªÜË∂äÂ•Ω')
#         # print("ok 340")

#         async for txt in model.chat_stream(tokenizer, prompt , history=history, system='‰Ω†ÊòØ‰∏Ä‰∏™Ê†πÊçÆÂ∑≤Áü•‰ø°ÊÅØËøõË°åÂõûÁ≠îÂíåÊÄªÁªìÂπ∂Ê∂¶Ëâ≤Á≠îÊ°àÁöÑÊú∫Âô®‰∫∫ÔºåÁªìÂêàÊùêÊñôÁöÑÊó∂ÂÄôÂ∞ΩÂèØËÉΩ‰øùÁïôÊùêÊñôÔºåÊÄªÁªìÊÄßÁöÑÂõûÁ≠îË∂äËØ¶ÁªÜË∂äÂ•Ω'):  
#             await websocket.send_text(txt[len(tmp):])
#         # for txt in res: 
#         #     print("### 342 ###")
#         #     # print(txt[len(tmp):], end='', flush=True)
#         #     await websocket.send_text(txt[len(tmp):])
#         #     print("### 345 ###")
#         #     tmp = txt
#         print('\n', end='')
#         await websocket.send_json({'ans': 'DONE'})
#         await websocket.close()
executor_enhance = ThreadPoolExecutor()  
@app.websocket('/api/enhance_socket')  
async def enhance_gpt(websocket: WebSocket):  
    await websocket.accept()  
  
    while True:  
        data = await websocket.receive_text()  
        item = Item(**json.loads(data))  
        prompt = retrieve(item.query)  
  
        # ‰ΩøÁî®Á∫øÁ®ãÊ±†ÊâßË°åÂô®Êù•ËøêË°åÂêåÊ≠•ÁîüÊàêÂô®  
        loop = asyncio.get_running_loop()  
        async for txt in run_in_executor(executor_enhance, model.chat_stream, tokenizer, prompt, history=history, system='‰Ω†ÊòØ‰∏Ä‰∏™Ê†πÊçÆÂ∑≤Áü•‰ø°ÊÅØËøõË°åÂõûÁ≠îÂíåÊÄªÁªìÂπ∂Ê∂¶Ëâ≤Á≠îÊ°àÁöÑÊú∫Âô®‰∫∫ÔºåÁªìÂêàÊùêÊñôÁöÑÊó∂ÂÄôÂ∞ΩÂèØËÉΩ‰øùÁïôÊùêÊñôÔºåÊÄªÁªìÊÄßÁöÑÂõûÁ≠îË∂äËØ¶ÁªÜË∂äÂ•Ω'):  
            await websocket.send_text(txt)  
  
        await websocket.send_json({'ans': 'DONE'})  
        # Â¶ÇÊûúÊâìÁÆóÂú®ÂèëÈÄÅDONEÂêéÁªßÁª≠ÁõëÂê¨Êñ∞ÁöÑËØ∑Ê±ÇÔºåÂ∞±‰∏çË¶ÅÂÖ≥Èó≠websocketËøûÊé•  
        # Â¶ÇÊûúËøôÊòØÂçïÊ¨°ËØ∑Ê±ÇÂìçÂ∫îÊ®°ÂºèÔºåÂàôÂèØ‰ª•ÂÖ≥Èó≠  
        await websocket.close()  

# ÂºÇÊ≠•ÂåÖË£ÖÂô®Êù•Âú®Á∫øÁ®ã‰∏≠ËøêË°åÂêåÊ≠•ÁîüÊàêÂô®  
async def run_in_executor(executor, fn, *args, **kwargs):  
    loop = asyncio.get_event_loop()  
    queue = asyncio.Queue()  
  
    def run():  
        try:  
            for item in fn(*args, **kwargs):  
                loop.call_soon_threadsafe(queue.put_nowait, item)  
            loop.call_soon_threadsafe(queue.put_nowait, None)  # Sentinel to stop iteration  
        except Exception as e:  
            loop.call_soon_threadsafe(queue.put_nowait, e)  
  
    executor.submit(run)  
  
    while True:  
        item = await queue.get()  
        if item is None:  # Sentinel detected  
            break  
        elif isinstance(item, Exception):  
            raise item  
        else:  
            yield item






#ËøîÂõûÁõ∏‰ººÂêëÈáèÁöÑÊé•Âè£
class Item(BaseModel):
    query: str
@app.post('/api/similartext/')
def similar_text(item: Item):
    res = query_db(item.query,topk=15)
    if res:
        texts='## üòÜÂ∑≤‰∏∫ÊÇ®Âú®Êú¨Âú∞Áü•ËØÜÂêëÈáèÊï∞ÊçÆÂ∫ìÊ£ÄÁ¥¢Âà∞Áõ∏‰ººÊñáÊú¨üõéÔ∏èüõéÔ∏èüöÄ  \n'
        texts+=f'*===Ê£ÄÁ¥¢ÂÜÖÂÆπ:{item.query}===*  \n'
        #ÂèñÂá∫Áõ∏‰ººÂú∞ÂêëÈáè
        for i,text in enumerate(res):
            texts+=f"**{i+1}."+str(text['text']).strip()+"**  \n"
        print(f"ËøôÊòØsimilar_textÁöÑtexts: {texts}")
        return {"code": 200, "similar_texts":texts}
    else:
        texts="## ü§îÊä±Ê≠âÔºåÊï∞ÊçÆÂ∫ì‰∏≠ÊöÇÊó∂Ê≤°ÊúâÊ£ÄÁ¥¢Âà∞Áõ∏‰ººÊñáÊú¨  \n**ÊÇ®ÂèØ‰ª•ÈÄâÊã©ËÅîÁ≥ªÁÆ°ÁêÜÂëòËøõË°å‰∏ä‰º†ËØ≠ÊñôÁöÑÊìç‰Ωú**"
        print(f"ËøôÊòØsimilar_textÁöÑtexts: {texts}")
        return {"code": 200, "similar_texts":texts}




#Êï∞ÊçÆÂ∫ìÊé•Âè£
@app.post('/api/upload/')
def insert_database(file: UploadFile = File(...)):
    with open('test.txt', 'wb') as f:
        f.write(file.file.read())

    #Âõ∫ÂÆöÁöÑÂÜôÊ≠ªÁöÑ
    #get_embedding("test.txt")
    # embedding, texts = read_embedding()
    embedding, texts=get_embedding("test.txt")
    print("****************************263********************")
    num = insert_embedding(embedding, texts)
    print('Êï∞ÊçÆ‰∏ä‰º†ÊàêÂäüÔºå‰∏ä‰º†%sÊù°ËØ≠ÊñôÔºÅÔºÅÔºÅ' % num)
    return {"code": 200,'msg':f'ÊàêÂäü‰∏ä‰º† {num} Êù°ËØ≠Êñô'}






@app.delete('/api/delete/')
def delete_collection():
    if utility.has_collection(collection_name):
        utility.drop_collection(collection_name)
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000)
    ]
    schema = CollectionSchema(fields, collection_name)
    collection = Collection(collection_name, schema)
    index_params = {
        "index_type": "IVF_FLAT",
        "metric_type": "IP",
        "params": {"nlist": 1536},
    }
    collection.create_index(field_name="embedding", index_params=index_params)
    collection.load()
    print("success Create collection: ", collection_name)


if __name__ == '__main__':
    # insert_cache_path = './dataset/insertdata_cache_3.json'
    # text2embedding(insert_cache_path)
    # file_path = './dataset/netse.txt'
    # openai_embedding(file_path)
    # embedding, texts = read_embedding()
    # num = insert_embedding(embedding, texts)
    # str = "ÊàëÁöÑ‰∏™‰∫∫‰ø°ÊÅØ‰ºöÂõ†‰∏∫ÁôªÂΩïÁΩëÁ´ôËÄåÊ≥ÑÈú≤Âêó"
    # query_str,prompt = retrieve(str)
    # print("prompt:",prompt)
    # print("---------Â∞èÂä©ÊâãÂõûÁ≠î----------")
    # res,all_price = chatgpt(query_str, prompt)
    # reply = res["choices"][0]["message"]["content"]
    # print(reply+'\n'+'ËØ•ÈóÆÈ¢òËä±Ë¥π %f ÁæéÂÖÉ'%all_price)
    uvicorn.run(app="main:app", host="0.0.0.0", port=9001, reload=True)
    signal.signal(signal.SIGCHLD, signal.SIG_IGN)


"""
while True:
    tmp = ''
    ask=input('user:')
    responds = model.chat_stream(tokenizer, ask,history=history,system='‰Ω†ÊòØ‰∏Ä‰∏™Áå´Â®òÔºåÊØèÊ¨°ÂõûÁ≠îÂÆåÈóÆÈ¢òË¶ÅÂñµ‰∏ÄÂ£∞')
    for txt in responds: 
        print(txt[len(tmp):],end='',flush=True)
        tmp = txt
        #print(1)
    print('\n',end='')
"""

# def openai_embedding(file_path,batch_size=100):
#     openai.api_key = 'sk-4aFYZcF2pcogyNgOCdDfE0D4E14b4d6eBdEe336772AdF6Be'
#     openai.api_base = "https://api.dreamger.com/v1"

#     print("ÂºÄÂßãËøõË°åopenai_embedding")
#     texts = []
#     res_feature = []
#     with open(file_path, encoding='utf-8') as f:
#         for line in f:
#             text = line.strip()
#             texts.append(text)

#     for i in tqdm(range(0, len(texts), batch_size)):
#         batch = texts[i:i + batch_size]
#         embed_data = openai.Embedding.create(input=batch, model=model_embedding)
#         feature = embed_data["data"]
#         print(feature)
#         for j in feature:
#             res_feature.append(j['embedding'])
#     print('ÊèêÂèñÁâπÂæÅÊï∞ÈáèÔºö',len(res_feature))
#     # with open('./dataset/insert_feature.json', 'w') as f:
#     #Ë¶ÜÁõñÂÜôÂÖ•Êñ∞ÁöÑembeddingÂêëÈáèÁÑ∂Âêé‰∏ä‰º†Âà∞Êï∞ÊçÆÂ∫ì‰∏≠
#     with open('insert_feature3.json', 'w') as f:
#         json.dump(res_feature, f)
#         print('ÊèêÂèñÂÆåÊàê')



# def estimate_embedding_price(text, price_per_1k=0.04):
#     enc = tiktoken.get_encoding("gpt2")
#     num_tokens = len(enc.encode(text))
#     price = num_tokens / 1000 * price_per_1k / 100
#     return num_tokens,price

#def enhance_chatgpt(prompt):
    # res = openai.ChatCompletion.create(
    #   model="gpt-4-turbo-2024-04-09",
    #   messages=[
    #         {"role": "system", "content": "ËØ∑Ê†πÊçÆÊâÄÁªô‰ø°ÊÅØÂõûÁ≠îÈóÆÈ¢ò"},
    #         {"role": "user", "content": prompt},
    #         #{"role": "assistant", "content": prompt}
    #     ]
    # )
    # query_str_tokens,query_price = estimate_embedding_price(query_str)
    # ans_tokens = res["usage"]["total_tokens"]
    # ans_price = ans_tokens / 1000 * 0.002 / 100
    # all_price = query_price + ans_price
    # return res,all_price



#‰Ω†ÊòØ‰∏™Âü∫Á°ÄÈóÆÁ≠îÊú∫Âô®‰∫∫
#def ori_chatgpt(query_str):
    # res = openai.ChatCompletion.create(
    #   model="gpt-4-turbo-2024-04-09",
    #   messages=[
    #         {"role": "system", "content": "‰Ω†ÊòØ‰∏™Âü∫Á°ÄÈóÆÁ≠îÊú∫Âô®‰∫∫"},
    #         {"role": "user", "content": query_str}
    #     ]
    # )
    # ans_tokens = res["usage"]["total_tokens"]
    # ans_price = ans_tokens / 1000 * 0.002 / 100
    # return res,ans_price
    # res = model.chat_stream(tokenizer, ask,history=history,system='‰Ω†ÊòØ‰∏™Âü∫Á°ÄÈóÆÁ≠îÊú∫Âô®‰∫∫,ËØ∑‰Ω†Ê†πÊçÆËá™Â∑±ÁöÑ‰ø°ÊÅØËøõË°åÂõûÁ≠îÔºåË∂äËØ¶ÁªÜË∂äÂ•Ω')
    # for txt in responds: 
    #     print(txt[len(tmp):],end='',flush=True)
    #     tmp = txt
    #     #print(1)
    # print('\n',end='')
    # history=None
    # res, history = model.chat(tokenizer, query_str, system="‰Ω†ÊòØ‰∏™Âü∫Á°ÄÈóÆÁ≠îÊú∫Âô®‰∫∫,ËØ∑‰Ω†Ê†πÊçÆËá™Â∑±ÁöÑ‰ø°ÊÅØËøõË°åÂõûÁ≠îÔºåË∂äËØ¶ÁªÜË∂äÂ•Ω",temperature=0.9, top_p=0.8,history=history)
    # return res

    # openai.api_key = 'sk-4aFYZcF2pcogyNgOCdDfE0D4E14b4d6eBdEe336772AdF6Be'
# openai.api_base = "https://api.dreamger.com/v1"

device2 = "cpu"
tokenizer_bert = BertTokenizer.from_pretrained('/home/zhengwen/huzhuoyue/minicpm/model/text2vec-base-chinese')
model_bert = BertModel.from_pretrained('/home/zhengwen/huzhuoyue/minicpm/model/text2vec-base-chinese').to(device2)
    # print("ÂºÄÂßãËøõË°åopenai_embedding")
    # texts = []
    # res_feature = []
    # with open(file_path, encoding='utf-8') as f:
    #     for line in f:
    #         text = line.strip()
    #         #['ÊñáÊú¨1','ÊñáÊú¨2','ÊñáÊú¨3'...]
    #         texts.append(text)
    
    # print(texts)
    # embed_data =[]
    # # for i in tqdm(range(0, len(texts), batch_size)):
    # #     #ÊñáÊú¨ÂàÜÊâπ
    # #     batch = texts[i:i + batch_size]
    # #     print("78")
    # #     # print("####### process_txts ######")
    # for text in texts:
    #     embed_data.append(process_texts([text]))
    #     print("82")
    #     print("#########  embed_data  ########")
    #     print(embed_data)
    
    # res_feature.extend(embed_data)
    # print('ÊèêÂèñÁâπÂæÅÊï∞ÈáèÔºö',len(res_feature))
    # print("########## res_feature#####")
    # # with open('./dataset/insert_feature.json', 'w') as f:
    # #Ë¶ÜÁõñÂÜôÂÖ•Êñ∞ÁöÑembeddingÂêëÈáèÁÑ∂Âêé‰∏ä‰º†Âà∞Êï∞ÊçÆÂ∫ì‰∏≠
    # with open('insert_feature.json', 'w') as f:
    #     json.dump(res_feature, f)
    #     print('ÊèêÂèñÂÆåÊàê')
    # return res_feature

    #     # Process each batch
    #     for text in batch:
    #         # Check if text length exceeds max length
    #         if len(text) > 512:
    #             split_texts = [text[j:j+512] for j in range(0, len(text), 512)]
    #             for split_text in split_texts:
    #                 embeddings_list.append(process_texts([split_text]))
    #         else:
    #             embeddings_list.append(process_texts([text]))

    #     res_feature.extend(embeddings_list)

    # print("Total embeddings processed:", len(res_feature))
    # print(res_feature)
    # with open('insert_feature.json', 'w') as f:
    #     json.dump(res_feature, f)
    #     print('ÊèêÂèñÂÆåÊàê')
    # return res_feature
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# def get_embedding(file_path, batch_size=1):
#     def process_texts(texts):
#         encoded_inputs = tokenizer_bert(texts, padding=True, truncation=True, return_tensors='pt')
#         model_outputs = model_bert(**encoded_inputs)
#         sentence_embeddings = mean_pooling(model_outputs, encoded_inputs['attention_mask'])
#         sentence_embeddings_norm = sentence_embeddings / sentence_embeddings.norm(p=2, dim=-1, keepdim=True)
#         sentence_embeddings_norm = sentence_embeddings_norm.squeeze(0)
#         return sentence_embeddings_norm.cpu().detach().numpy().tolist()
    
#     texts = []  
#     max_length = 400
#     res_feature=[]

#     with open(file_path, encoding='utf-8') as f:
#         for line in f:
#             text = line.strip()
#             #texts.append(text)
#             if len(text) > max_length:  
#                 split_texts = [text[j:j+max_length] for j in range(0, len(text), max_length)]
#                 for split_text in split_texts:
#                     texts.append(split_text)
#             else:  
#                 texts.append(text)  # ÊñáÊú¨ÈïøÂ∫¶Êú™Ë∂ÖËøá512ÔºåÁõ¥Êé•Ê∑ªÂä†Âà∞textsÂàóË°®‰∏≠  
#     texts=[text for text in texts if text]
#     print("######### texts  #########")
#     print(texts)
#     embeddings_list = []
#     print("##### ok 131 ###")
#     for item in texts:
#         print("##### ok 133 ###")
#     # for i in tqdm(range(0, len(texts), batch_size)):
#     #     #‰∏ÄÊâπ‰∏ÄÊâπÂ§ÑÁêÜÊñáÊú¨
#     #     batch = texts[i:i + batch_size]
#         embeddings_list.append(process_texts([item]))
#         #ÂΩìÂâçbatchÂà§Êñ≠ÈïøÂ∫¶ÊòØÂê¶Â§ß‰∫é512
#         # for text in batch:
#         #     # Check if text length exceeds max length
#         #     if len(text) > 512:
#         #         #ÈïøÂ∫¶Â§ß‰∫é512ÈáçÊñ∞ÂàáÁâá
#         #         split_texts = [text[j:j+512] for j in range(0, len(text), 512)]
#         #         for split_text in split_texts:
#         #             embeddings_list.append(process_texts([split_text]))
#         #     else:
#         #         embeddings_list.append(process_texts([text]))
#     res_feature.extend(embeddings_list)
#     print("##### ok 147 ###")
#     print("Total embeddings processed:", len(res_feature))
#     print(res_feature)
#     with open('insert_feature.json', 'w') as f:
#         json.dump(res_feature, f)
#         print('ÊèêÂèñÂÆåÊàê')
#     return res_feature,texts
